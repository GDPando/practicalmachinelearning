---
title: "Practical Machine Learning Course Project"
author: "Gonzalo Delgado"
date: "29/4/2020"
output: html_document
---

## Executive summary
In this project I work with Weight Lifting Exercise Dataset obtained from  http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. I construct different models with the training set to then predict the manner of how 20 individuals did the exercise (classe variable) with results compiled on the testing dataset. The use of random forest as the method for modelling the dataset gave the best accuracy in comparison with other models. For this reason the prediction is being done with this model. 


### Data cleaning and exploratory Analysis
```{r message=FALSE, warning=FALSE, include=FALSE}
setwd("~/Coursera/Data Science Statistics and Machine Learning Specialization/Practical Machine Learning/Gitrepo")
train<-read.csv("pml-training.csv")
test<-read.csv("pml-testing.csv")
```
The data has 160 vbles and 19622 observations so it comprises a really big dataset and number of variables to work with. Quite a few of the columns have NA on them and that the first 7 columns are not related with classe, the variable we want to predict. I will remove all of this columns from training and testing datasets
```{r}
train <- train[, which(colSums(is.na(train)) == 0)] 
test <- test[, which(colSums(is.na(test)) == 0)]
train <- train[,-c(1:7)] 
test <- test[,-c(1:7)]
```
From 160 variables on the training set only 86 are now part of it. As many of them would be correlated due to the nature of the measurements I use the near zero variance to remove those predictors that wonÂ´t give extra information.

```{r message=FALSE, warning=FALSE}
library(caret)
nzv <- nearZeroVar(train)
train<-train[,-nzv]
```

## Subsetting the dataset
Next thing to do is subset the train set into training and testing sets so I can observe how good or bad the models are. I split the test set 70% to training and 30% to testing
```{r}
testIndex = createDataPartition(train$classe, p = 0.70,list=FALSE)
training = train[testIndex,]
testing = train[-testIndex,]
```

## Model Building
I perform a 5-fold cross validation on the `training`dataset and build it on three different models: random forest, linear discriminant analysis and boosting.

```{r}
set.seed(383)
ModRF <- train(classe~., method="rf", data = training, trControl=trainControl(method="cv", number=5))
ModLDA <- train(classe~., method="lda", data = training, trControl=trainControl(method="cv", number=5))
ModGBM <- train(classe~., method="gbm", data = training, verbose=F, trControl=trainControl(method="cv", number=5))

```
Now we can check the Confussion matrix on the prediction of the testing set we split before.

#### Random Forest
```{r}
predRF <- predict(ModRF, testing)
confusionMatrix(predRF, testing$classe)
```
The model is quite good in predicting the testing set with an accuracy of 99%.

#### Linear Discriminant Analysis (LDA)
```{r}
predLDA <- predict(ModLDA, testing)
confusionMatrix(predLDA, testing$classe)
```
The accuracy on the testing set is not very good

#### Gradient Boosting Algorithm (GBM)
```{r}
predGBM <- predict(ModGBM, testing)
confusionMatrix(predGBM, testing$classe)
```
This model has also really high accuracy on the testing set but it is not as good as the random forest one.


## Prediction
I select the random forest model as it had the best accuracy from the three models. Then I apply this model to the test set to obtain the predicted values for the "classe" of these individuals.
```{r}
predTest <- predict(ModRF, test)
DF<-data.frame(test$problem_id, predTest)
colnames(DF)<-c("Problem ID","Predicted Class")
DF
```

